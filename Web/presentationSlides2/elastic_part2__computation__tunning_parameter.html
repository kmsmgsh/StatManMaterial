<!DOCTYPE html>
<html>
  <head>
    <title>Elastic Net Part2</title>
    <meta charset="utf-8">
    <meta name="author" content="Jiaming Shen" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Elastic Net Part2
### Jiaming Shen
### updated: 2018-10-10

---


#Elastic Net Modification of LARS



As we assumed before, LARS is a efficient method to solve Lasso-type 
By lemma 1, for each fixed `\(\lambda_2\)` the elastic net problem is equivalent to a lasso problem on the augmented data set. So the entire elastic net solution path can be efficiently computed by LARS. The only problem that should be cared is `\(p &gt;&gt; n\)` case may slow down the computation significantly.


A key differentce between LARS and LARS-EN is the The matrix using to calculate the angle-bisector is 

`\(G=X^TX\)` , but for elastic net, the augumented data set, `\(G^*\)` comes to `\(G_{A_k}^* =X^{*T}_{A_{k}} X^*_{A_k}\)` `\(\mathbf { G }^* _ { A } = \frac { 1 } { 1 + \lambda _ { 2 } } \left( \mathbf { X } _ { A } ^ { \mathrm { T } } \mathbf { X } _ { A } + \lambda _ { 2 } \mathbf { I } \right)\)`  . That is the form of LARS-EN. We need calculate the inverse of this new matrix.   

As the LARS-Lasso with condition "one at a time", the updating or downdating of case one involve one covariates in each step.   

A simple formula to update the Cholesky factorization of `\(\mathbf { X } _ { A _ { k - 1 } } ^ { \mathrm { T } } \mathbf { X } _ { A _ { k - 1 } } + \lambda _ { 2 } \mathbf { I }\)` which is very similar to updating the Cholesky factorization for `\(\mathbf { X } _ { A _ { k - 1 } } ^ { \mathrm { T } } \mathbf { X } _ { A _ { k - 1 } }\)` (Golub and Van Loan, 1983) .  

---

In addition, when calculating the equiangular vector and the inner products of the nonactive predictors with the current residuals, we can save computations by using the simple fact that `\(X^*_j\)` has `\(p-1\)` zero elements. That is, do not explicitly use `\(X^*\)` to ccompute all the quantities in algorithm LARS. It is also economical to record only the non-zero coefficients and the active variables set at each LARS-EN step.

LARS-EN also do not need run to end. Especially for p&gt;&gt;n casae. Experiments show that the optimal results are achieved at an early stage.

---


# Choice of tuning parameters.

### 3.5.1 Lasso conventional tuning parameterï¼š

There are several ways to model the elastic net.

- Use `\(\lambda_1,\lambda_2\)`  Parameter the elastic net, but this is not the only way.

- Remember that `\(\hat \beta ^*\)` and  `\(\hat\beta\)` have the proportional relationship. We can model it with `\((\lambda,s)\)` or `\((\lambda,t)\)` , which `\(t\)`  is the `\(L_1\)` - norm of the coefficient, s is the fraction of the `\(L_1\)`-Norm.

  The benefits of using `\(s\)` is that `\(s\)` is always valued within `\([0,1]\)` .

-&gt; In algorithm LARS the lasso is described as a forward stagewise additive fitting procedure and shown to be (almost) identical to `\(\epsilon-L_2\)` boosting. 

  In the view pointed out upon, the number of steps k of algorith LARS also is tuning parameter for the lasso. That is, for fixed `\(\lambda_2\)` , the elastic net is solved by LARS-EN; hence similarly, we can use the number of the LARS-EN steps (k) as the second tuning parameter besides `\(\lambda_2\)`. 

---


## 3.5.2 Chose tuning parameter.

Only training data are available: tenfold cross-validation.

Because there are two-tuning parameters, we need to cross-validate on a two-dimensional surface. Typically we first pick a (relatively small) grid of values for `\(\lambda_2\)` , say `\((0,0.01,0.1,1,10,100)\)`. Then for each `\(\lambda_2\)` , algorithm LARS-EN produces the entire solution path of the elastic net. 

In this way, the other tuning parameter `\((\lambda_1,s,\text{or},k)\)` is selected by tenfold CV. The chosen `\(\lambda_2\)` is the one giving the smallest CV error.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
